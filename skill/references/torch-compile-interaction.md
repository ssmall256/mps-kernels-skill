# torch.compile Interaction with Custom MPS Kernels

## Core Rule

`torch.compile` can optimize Python-level graph structure around your kernels,
but it does not optimize kernel internals generated by `compile_shader`.

## When It Helps

Use `torch.compile` when your forward path has:

- multiple surrounding torch ops that can fuse/reorder
- repeated shape patterns where graph capture amortizes

## When It Does Not Help Much

If your function is mostly one custom kernel call, graph capture overhead may
provide little value.

## Shape Specialization Notes

- graph capture may specialize to shape/dtype sets
- excessive dynamic shapes can trigger recompilation churn

Prefer bounded shape buckets in performance-critical paths.

## Practical Advice

1. validate correctness without `torch.compile` first
2. benchmark baseline vs custom kernel, both eager and compiled
3. keep whichever mode is consistently faster in end-to-end workload
